{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SfJAi2pF5PTC"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Description:\n",
        "\n",
        "Breast cancer is the most common cancer amongst women in the world. It accounts for 25% of all cancer cases, and affected over 2.1 Million people in 2015 alone. It starts when cells in the breast begin to grow out of control. These cells usually form tumors that can be seen via X-ray or felt as lumps in the breast area.\n",
        "\n",
        "The idea here is to classify tumors into malignant (cancerous) or benign(non-cancerous)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer #using the breast cancer dataset \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import accuracy_score\n",
        "#building a classification tree from scratch and only using numpy, train_test_split and accuracy_score in this practice\n",
        "\n",
        "def entropy(y):\n",
        "    unique, counts = np.unique(y, return_counts=True) #count the unique value occurence in y\n",
        "    p = counts / len(y) #where p is the prob of the unique value in y\n",
        "    return -np.sum(p * np.log2(p)) #the entropy euqation is this \n",
        "\n",
        "def get_gain(X, y, feature_idex, threshold):\n",
        "    left_idexs = X[:, feature_idex] < threshold #if value is smaller than threshold, assign to the left\n",
        "    right_idexs = X[:, feature_idex] >= threshold #otherwise, assign to the right\n",
        "\n",
        "    number_left, number_right = len(y[left_idexs]), len(y[right_idexs]) #check the number of exaples on left and right split \n",
        "    n_total = number_left + number_right\n",
        "\n",
        "    if number_left == 0 or number_right == 0:\n",
        "        return 0 #if either side does not have any example, it is not good for learning, then the info gain is 0\n",
        "\n",
        "    gain = entropy(y) - (number_left/n_total)*entropy(y[left_idexs]) - (number_right/n_total)*entropy(y[right_idexs])\n",
        "    #otherwise, we calculate the info gain by taking the entropy of parent node - weighted entropy left - weighted entropy right\n",
        "\n",
        "    return gain\n",
        "\n",
        "def get_best_feature(X, y, feature_idexs):\n",
        "    best_gain = -1 #initialise best gain\n",
        "    split_idex, split_threshold = None, None\n",
        "\n",
        "    for feature_idex in feature_idexs:\n",
        "        thresholds = np.unique(X[:, feature_idex]) #go through each feature in X\n",
        "        for threshold in thresholds:\n",
        "            gain = get_gain(X, y, feature_idex, threshold) #within each feature, set each unique value to threshold and calculate the info gain\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain \n",
        "                split_idex = feature_idex\n",
        "                split_threshold = threshold\n",
        "\n",
        "    return split_idex, split_threshold #eventually, we will get the best index to split and the best threshold for that feature \n",
        "\n",
        "def build_tree(X, y, depth=0, max_depth=10, min_samples_split=2):\n",
        "    n_samples = X.shape[0] #the number of samples I have in the dataset \n",
        "    n_labels = len(np.unique(y)) # the unique labels in the dataset \n",
        "\n",
        "    #stop conditions\n",
        "    #there are three scenarios: \n",
        "    #when current depth is exceeding max depth (default is 10)\n",
        "    #when all labels in the split is the same (meaning unique value of labe in the node = 1)\n",
        "    #when current split has fewer samples than min_samples_split\n",
        "    if depth >= max_depth or n_labels == 1 or n_samples < min_samples_split:\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        most_common_label = unique[np.argmax(counts)]\n",
        "        return {'leaf_value': most_common_label} #then return a leaf node with a predicted label\n",
        "\n",
        "    # Find the best feature to split on:\n",
        "    feature_idexs = np.random.choice(X.shape[1], int(np.sqrt(X.shape[1])), replace=False) \n",
        "    #size of randomly selected features is the square root of the number of features rounded down to the nearest integer\n",
        "    best_feature, best_threshold = get_best_feature(X, y, feature_idexs)\n",
        "    #use get_best_feature function to get the best feature and the threshold\n",
        "\n",
        "    #split data to left and right based on best feature and threshold\n",
        "    left_idexs = X[:, best_feature] < best_threshold\n",
        "    right_idexs = X[:, best_feature] >= best_threshold\n",
        "\n",
        "    #build tree from the left and right index\n",
        "    left = build_tree(X[left_idexs], y[left_idexs], depth+1, max_depth, min_samples_split)\n",
        "    right = build_tree(X[right_idexs], y[right_idexs], depth+1, max_depth, min_samples_split)\n",
        "\n",
        "    #create a dictionary to represent the current node, that is best feature, best threshold, the left and right sub tree\n",
        "    return {'feature_idx': best_feature, 'threshold': best_threshold,\n",
        "            'left': left, 'right': right}\n",
        "\n",
        "def predict(X, tree):\n",
        "    def traverse_tree(x, node):\n",
        "        if 'leaf_value' in node:\n",
        "            return node['leaf_value'] #if this is a leaf node then tree should return a predicted label \n",
        "\n",
        "        if x[node['feature_idx']] < node['threshold']:\n",
        "            return traverse_tree(x, node['left'])\n",
        "        else:\n",
        "            return traverse_tree(x, node['right'])\n",
        "        #either going to the left or to the right depending on the value and the threshold\n",
        "\n",
        "    return np.array([traverse_tree(x, tree) for x in X])\n",
        "\n",
        "def print_tree(node, feature_names, target_names, depth=0):\n",
        "    indent = '  ' * depth\n",
        "    if 'leaf_value' in node:\n",
        "        target_name = target_names[node['leaf_value']]\n",
        "        print(f\"{indent}*[{target_name}]\")\n",
        "    else:\n",
        "        feature_name = feature_names[node['feature_idx']]\n",
        "        print(f\"{indent}[{feature_name} < {node['threshold']:.3f}]\")\n",
        "        print_tree(node['left'], feature_names, target_names, depth+1)\n",
        "        print_tree(node['right'], feature_names, target_names, depth+1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load the breast cancer dataset (breast cancer = bc)\n",
        "X_bc, y_bc = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "#split the dataset into training and testing sets\n",
        "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "\n",
        "#build the decision tree with predefined function build tree \n",
        "tree_bc = build_tree(X_train_bc, y_train_bc, max_depth=5)\n",
        "\n",
        "#make predictions on the testing set\n",
        "y_pred_bc = predict(X_test_bc, tree_bc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Breast cancer dataset accuracy: 0.947\n"
          ]
        }
      ],
      "source": [
        "#calculate accuracy of the model on the testing set\n",
        "accuracy_bc = accuracy_score(y_test_bc, y_pred_bc)\n",
        "print(f\"Breast cancer dataset accuracy: {accuracy_bc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The features include:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension'] \n",
            "\n",
            "The labels include:  ['malignant' 'benign']\n"
          ]
        }
      ],
      "source": [
        "print('The features include: ', load_breast_cancer().feature_names, '\\n') #this is the features we have, a total of 30. \n",
        "print('The labels include: ', load_breast_cancer().target_names) #this is what we are predicting. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[worst concave points < 0.142]\n",
            "  [mean area < 698.800]\n",
            "    [worst area < 728.300]\n",
            "      [texture error < 1.380]\n",
            "        *[benign]\n",
            "        [compactness error < 0.012]\n",
            "          *[benign]\n",
            "          *[benign]\n",
            "      [mean texture < 19.630]\n",
            "        *[benign]\n",
            "        [symmetry error < 0.015]\n",
            "          *[malignant]\n",
            "          *[benign]\n",
            "    [worst radius < 19.850]\n",
            "      [mean texture < 16.680]\n",
            "        *[benign]\n",
            "        [mean texture < 20.200]\n",
            "          *[malignant]\n",
            "          *[malignant]\n",
            "      *[malignant]\n",
            "  [mean perimeter < 71.900]\n",
            "    *[benign]\n",
            "    [worst area < 869.300]\n",
            "      [worst texture < 27.570]\n",
            "        [smoothness error < 0.007]\n",
            "          *[benign]\n",
            "          *[malignant]\n",
            "        *[malignant]\n",
            "      *[malignant]\n"
          ]
        }
      ],
      "source": [
        "#print the tree for visualisation \n",
        "print_tree(tree_bc, load_breast_cancer().feature_names, load_breast_cancer().target_names)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5i4folxZ8_-V"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the mean squared error (MSE) function\n",
        "def mse(y):\n",
        "    return np.mean((y - np.mean(y))**2)\n",
        "\n",
        "# Define a function to split the data into left and right subsets based on a threshold\n",
        "def split(X, y, feature_idx, threshold):\n",
        "    left_indices = np.where(X[:, feature_idx] <= threshold)[0]\n",
        "    right_indices = np.where(X[:, feature_idx] > threshold)[0]\n",
        "    if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "        return None, None, None, None\n",
        "    else:\n",
        "        return X[left_indices], y[left_indices], X[right_indices], y[right_indices]\n",
        "\n",
        "# Define a function to find the best split based on minimizing the MSE\n",
        "def best_split(X, y, min_samples_split_set=2):\n",
        "    best_feature_idx, best_threshold, best_mse = None, None, np.inf\n",
        "    # Loop through all the features in X\n",
        "    for feature_idx in range(X.shape[1]):\n",
        "        # Find all the unique values of the feature\n",
        "        thresholds = np.unique(X[:, feature_idx])\n",
        "        # Loop through all the unique values of the feature\n",
        "        for threshold in thresholds:\n",
        "            # Split the data into left and right subsets based on the threshold\n",
        "            X_left, y_left, X_right, y_right = split(X, y, feature_idx, threshold)\n",
        "            # Check that the split is valid (i.e. neither the left nor right subset is empty)\n",
        "            if y_left is not None and y_right is not None:\n",
        "                # Check that the number of samples in each subset is greater than or equal to min_samples_split_set\n",
        "                if len(y_left) < min_samples_split_set or len(y_right) < min_samples_split_set:\n",
        "                    continue\n",
        "                # Calculate the total MSE of the left and right subsets\n",
        "                total_mse = mse(y_left) + mse(y_right)\n",
        "                # Update the best split if the total MSE is lower than the current best MSE\n",
        "                if total_mse < best_mse:\n",
        "                    best_feature_idx, best_threshold, best_mse = feature_idx, threshold, total_mse\n",
        "    return best_feature_idx, best_threshold\n",
        "# Define a function to build the decision tree recursively\n",
        "def build_tree(X, y, depth, max_depth, min_samples_split):\n",
        "    # Check if the maximum depth has been reached or if there is no further reduction in MSE\n",
        "    if depth == max_depth or mse(y) == 0 or len(X) < min_samples_split:\n",
        "        # Return the mean target value\n",
        "        return np.mean(y)\n",
        "    # Find the best split based on minimizing the MSE\n",
        "    feature_idx, threshold = best_split(X, y)\n",
        "    # Check if there is no valid split\n",
        "    if feature_idx is None:\n",
        "        # Return the mean target value\n",
        "        return np.mean(y)\n",
        "    # Split the data into left and right subsets based on the best split\n",
        "    else:\n",
        "        X_left, y_left, X_right, y_right = split(X, y, feature_idx, threshold)\n",
        "        # Recursively build the left and right subtrees\n",
        "        left_tree = build_tree(X_left, y_left, depth+1, max_depth, min_samples_split)\n",
        "        right_tree = build_tree(X_right, y_right, depth+1, max_depth, min_samples_split)\n",
        "        # Return the decision node with the best split and the left and right subtrees\n",
        "        return (feature_idx, threshold, left_tree, right_tree)\n",
        "# Define a function to make predictions for a single input using the decision tree\n",
        "\n",
        "\n",
        "def predict_one(x, tree):\n",
        "    # Check if the current node is a leaf node (i.e. a float value)\n",
        "    if isinstance(tree, float):\n",
        "        # Return the mean target value of the leaf node\n",
        "        return tree\n",
        "    \n",
        "    # Check which subtree to go down based on the feature value of the current data point\n",
        "    else:\n",
        "        feature_idx, threshold, left_tree, right_tree = tree\n",
        "        if x[feature_idx] <= threshold:\n",
        "            # Recursively go down the left subtree\n",
        "            return predict_one(x, left_tree)\n",
        "        else:\n",
        "            # Recursively go down the right subtree\n",
        "            return predict_one(x, right_tree)\n",
        "\n",
        "\n",
        "# A function to predict the target values of multiple data points using a decision tree\n",
        "def predict(X, tree):\n",
        "    # Predict the target value of each data point using the predict_one function\n",
        "    return np.array([predict_one(x, tree) for x in X])\n",
        "\n",
        "\n",
        "# A function to build a decision tree for regression\n",
        "def decision_tree_regression(X_train, y_train, max_depth, min_samples_split_set):\n",
        "    # Build the decision tree using the build_tree function\n",
        "    tree = build_tree(X_train, y_train, 0, max_depth, min_samples_split_set)\n",
        "    # Predict the target values of the training set using the predict function\n",
        "    y_pred = predict(X_train, tree)\n",
        "    # Return the decision tree and the predicted target values of the training set\n",
        "    return tree, y_pred\n",
        "\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "max_depth = 2\n",
        "min_samples_split_set = 10\n",
        "tree, y_pred_train = decision_tree_regression(X_train, y_train, max_depth, min_samples_split_set)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_test = predict(X_test, tree)\n",
        "\n",
        "# Print the mean squared error on the testing set\n",
        "print(\"Mean squared error on the testing set:\", mse(y_test - y_pred_test))\n",
        "# Calculate root mean squared error of the model on the testing set\n",
        "rmse_db = np.sqrt(mse(y_test - y_pred_test))\n",
        "print(f\"Diabetes dataset RMSE: {rmse_db:.3f}\")\n",
        "\n",
        "def print_tree(node, feature_names, target_names, depth=0):\n",
        "    indent = '  ' * depth\n",
        "    if isinstance(node, float):\n",
        "        print(f\"{indent}*[{node:.3f}]\")\n",
        "    else:\n",
        "        feature_name = feature_names[node[0]]\n",
        "        print(f\"{indent}[{feature_name} < {node[1]:.3f}]\")\n",
        "        print_tree(node[2], feature_names, target_names, depth+1)\n",
        "        print_tree(node[3], feature_names, target_names, depth+1)\n",
        "\n",
        "# Build the decision tree and print it\n",
        "tree = build_tree(X, y, 0, max_depth=2,min_samples_split=20)\n",
        "target_names = ['target']\n",
        "feature_names = diabetes.feature_names\n",
        "print_tree(tree, feature_names, target_names)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
