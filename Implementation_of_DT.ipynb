{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "With classes"
      ],
      "metadata": {
        "id": "SfJAi2pF5PTC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ux3w75ZM5G5K"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=10, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features_ = X.shape[1]\n",
        "        self.tree_ = self.build_tree(X, y)\n",
        "\n",
        "    def build_tree(self, X, y, depth=0):\n",
        "        n_samples = X.shape[0]\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # Stop conditions:\n",
        "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
        "            leaf_value = self.get_leaf_value(y)\n",
        "            return {'leaf_value': leaf_value}\n",
        "\n",
        "        # Find the best feature to split on:\n",
        "        feature_idxs = np.random.choice(self.n_features_, int(np.sqrt(self.n_features_)), replace=False)\n",
        "        best_feature, best_threshold = self.get_best_feature(X, y, feature_idxs)\n",
        "\n",
        "        # Split data based on best feature and threshold:\n",
        "        left_idxs = X[:, best_feature] < best_threshold\n",
        "        right_idxs = X[:, best_feature] >= best_threshold\n",
        "        left = self.build_tree(X[left_idxs], y[left_idxs], depth+1)\n",
        "        right = self.build_tree(X[right_idxs], y[right_idxs], depth+1)\n",
        "\n",
        "        return {'feature_idx': best_feature, 'threshold': best_threshold,\n",
        "                'left': left, 'right': right}\n",
        "\n",
        "    def get_best_feature(self, X, y, feature_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feature_idx in feature_idxs:\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                gain = self.get_gain(X, y, feature_idx, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feature_idx\n",
        "                    split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def get_gain(self, X, y, feature_idx, threshold):\n",
        "        left_idxs = X[:, feature_idx] < threshold\n",
        "        right_idxs = X[:, feature_idx] >= threshold\n",
        "\n",
        "        n_left, n_right = len(y[left_idxs]), len(y[right_idxs])\n",
        "        n_total = n_left + n_right\n",
        "\n",
        "        if n_left == 0 or n_right == 0:\n",
        "            return 0\n",
        "\n",
        "        gain = self.entropy(y) - (n_left/n_total)*self.entropy(y[left_idxs]) - (n_right/n_total)*self.entropy(y[right_idxs])\n",
        "\n",
        "        return gain\n",
        "\n",
        "    def get_leaf_value(self, y):\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        most_common_label = unique[np.argmax(counts)]\n",
        "        return most_common_label\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.traverse_tree(x, self.tree_) for x in X])\n",
        "\n",
        "    def traverse_tree(self, x, node):\n",
        "        if 'leaf_value' in node:\n",
        "            return node['leaf_value']\n",
        "\n",
        "        if x[node['feature_idx']] < node['threshold']:\n",
        "            return self.traverse_tree(x, node['left'])\n",
        "        else:\n",
        "            return self.traverse_tree(x, node['right'])\n",
        "\n",
        "    def entropy(self, y):\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        p = counts / len(y)\n",
        "        return -np.sum(p * np.log2(p))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "X_bc, y_bc = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the decision tree model\n",
        "dt_bc = DecisionTree(max_depth=5)\n",
        "dt_bc.fit(X_train_bc, y_train_bc)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_bc = dt_bc.predict(X_test_bc)\n",
        "\n",
        "# Calculate accuracy of the model on the testing set\n",
        "acc_bc = accuracy_score(y_test_bc, y_pred_bc)\n",
        "print(f\"Breast cancer dataset accuracy: {acc_bc:.3f}\")\n",
        "\n",
        "# Load the diabetes dataset\n",
        "X_db, y_db = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_db, X_test_db, y_train_db, y_test_db = train_test_split(X_db, y_db, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the decision tree model\n",
        "dt_db = DecisionTree(max_depth=5)\n",
        "dt_db.fit(X_train_db, y_train_db)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_db = dt_db.predict(X_test_db)\n",
        "\n",
        "# Calculate mean squared error of the model on the testing set\n",
        "mse_db = mean_squared_error(y_test_db, y_pred_db)\n",
        "print(f\"Diabetes dataset MSE: {mse_db:.3f}\")\n",
        "\n",
        "# Calculate root mean squared error of the model on the testing set\n",
        "rmse_db = np.sqrt(mse_db)\n",
        "print(f\"Diabetes dataset RMSE: {rmse_db:.3f}\")\n",
        "\n",
        "# Calculate mean absolute error of the model on the testing set\n",
        "mae_db = mean_absolute_error(y_test_db, y_pred_db)\n",
        "print(f\"Diabetes dataset MAE: {mae_db:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DswR3ca75Tt-",
        "outputId": "8e4b76e5-c594-4e0c-b460-120dbd79476e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breast cancer dataset accuracy: 0.947\n",
            "Diabetes dataset MSE: 7942.416\n",
            "Diabetes dataset RMSE: 89.120\n",
            "Diabetes dataset MAE: 69.360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression"
      ],
      "metadata": {
        "id": "5i4folxZ8_-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the mean squared error (MSE) function\n",
        "def mse(y):\n",
        "    return np.mean((y - np.mean(y))**2)\n",
        "\n",
        "# Define a function to split the data into left and right subsets based on a threshold\n",
        "def split(X, y, feature_idx, threshold):\n",
        "    left_indices = np.where(X[:, feature_idx] <= threshold)[0]\n",
        "    right_indices = np.where(X[:, feature_idx] > threshold)[0]\n",
        "    if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "        return None, None, None, None\n",
        "    else:\n",
        "        return X[left_indices], y[left_indices], X[right_indices], y[right_indices]\n",
        "\n",
        "# Define a function to find the best split based on minimizing the MSE\n",
        "def best_split(X, y):\n",
        "    best_feature_idx, best_threshold, best_mse = None, None, np.inf\n",
        "    # Loop through all the features in X\n",
        "    for feature_idx in range(X.shape[1]):\n",
        "        # Find all the unique values of the feature\n",
        "        thresholds = np.unique(X[:, feature_idx])\n",
        "        # Loop through all the unique values of the feature\n",
        "        for threshold in thresholds:\n",
        "            # Split the data into left and right subsets based on the threshold\n",
        "            X_left, y_left, X_right, y_right = split(X, y, feature_idx, threshold)\n",
        "            # Check that the split is valid (i.e. neither the left nor right subset is empty)\n",
        "            if y_left is not None and y_right is not None:\n",
        "                # Calculate the total MSE of the left and right subsets\n",
        "                total_mse = mse(y_left) + mse(y_right)\n",
        "                # Update the best split if the total MSE is lower than the current best MSE\n",
        "                if total_mse < best_mse:\n",
        "                    best_feature_idx, best_threshold, best_mse = feature_idx, threshold, total_mse\n",
        "    return best_feature_idx, best_threshold\n",
        "\n",
        "# Define a function to build the decision tree recursively\n",
        "def build_tree(X, y, depth, max_depth):\n",
        "    # Check if the maximum depth has been reached or if there is no further reduction in MSE\n",
        "    if depth == max_depth or mse(y) == 0:\n",
        "        # Return the mean target value\n",
        "        return np.mean(y)\n",
        "    # Find the best split based on minimizing the MSE\n",
        "    feature_idx, threshold = best_split(X, y)\n",
        "    # Check if there is no valid split\n",
        "    if feature_idx is None:\n",
        "        # Return the mean target value\n",
        "        return np.mean(y)\n",
        "    # Split the data into left and right subsets based on the best split\n",
        "    else:\n",
        "        X_left, y_left, X_right, y_right = split(X, y, feature_idx, threshold)\n",
        "        # Recursively build the left and right subtrees\n",
        "        left_tree = build_tree(X_left, y_left, depth+1, max_depth)\n",
        "        right_tree = build_tree(X_right, y_right, depth+1, max_depth)\n",
        "        # Return the decision node with the best split and the left and right subtrees\n",
        "        return (feature_idx, threshold, left_tree, right_tree)\n",
        "\n",
        "# Define a function to make predictions for a single input using the decision tree\n",
        "\n",
        "\n",
        "def predict_one(x, tree):\n",
        "    # Check if the current node is a leaf node (i.e. a float value)\n",
        "    if isinstance(tree, float):\n",
        "        # Return the mean target value of the leaf node\n",
        "        return tree\n",
        "    \n",
        "    # Check which subtree to go down based on the feature value of the current data point\n",
        "    else:\n",
        "        feature_idx, threshold, left_tree, right_tree = tree\n",
        "        if x[feature_idx] <= threshold:\n",
        "            # Recursively go down the left subtree\n",
        "            return predict_one(x, left_tree)\n",
        "        else:\n",
        "            # Recursively go down the right subtree\n",
        "            return predict_one(x, right_tree)\n",
        "\n",
        "\n",
        "# A function to predict the target values of multiple data points using a decision tree\n",
        "def predict(X, tree):\n",
        "    # Predict the target value of each data point using the predict_one function\n",
        "    return np.array([predict_one(x, tree) for x in X])\n",
        "\n",
        "\n",
        "# A function to build a decision tree for regression\n",
        "def decision_tree_regression(X_train, y_train, max_depth):\n",
        "    # Build the decision tree using the build_tree function\n",
        "    tree = build_tree(X_train, y_train, 0, max_depth)\n",
        "    # Predict the target values of the training set using the predict function\n",
        "    y_pred = predict(X_train, tree)\n",
        "    # Return the decision tree and the predicted target values of the training set\n",
        "    return tree, y_pred\n",
        "\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the decision tree regression model\n",
        "max_depth = 5\n",
        "tree, y_pred_train = decision_tree_regression(X_train, y_train, max_depth)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_test = predict(X_test, tree)\n",
        "\n",
        "# Print the mean squared error on the testing set\n",
        "print(\"Mean squared error on the testing set:\", mse(y_test - y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRi5Lm7_9CDy",
        "outputId": "eb0ed6e2-d17e-47b3-9c1f-3a5c23542283"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error on the testing set: 5978.60270528184\n"
          ]
        }
      ]
    }
  ]
}