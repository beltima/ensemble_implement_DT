{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfJAi2pF5PTC"
      },
      "source": [
        "With classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Breast cancer dataset accuracy: 0.939\n",
            "[X6 < 0.073]\n",
            "  [X13 < 43.400]\n",
            "    [X0 < 15.100]\n",
            "      [X20 < 14.490]\n",
            "        [1]\n",
            "        [X0 < 13.620]\n",
            "          [1]\n",
            "          [1]\n",
            "      [X21 < 26.560]\n",
            "        [1]\n",
            "        [0]\n",
            "    [X29 < 0.069]\n",
            "      [X14 < 0.005]\n",
            "        [1]\n",
            "        [X0 < 14.990]\n",
            "          [1]\n",
            "          [0]\n",
            "      [0]\n",
            "  [X3 < 701.900]\n",
            "    [X25 < 0.368]\n",
            "      [X15 < 0.030]\n",
            "        [X1 < 19.620]\n",
            "          [1]\n",
            "          [0]\n",
            "        [X18 < 0.052]\n",
            "          [1]\n",
            "          [0]\n",
            "      [X23 < 739.300]\n",
            "        [X21 < 26.380]\n",
            "          [1]\n",
            "          [0]\n",
            "        [X4 < 0.095]\n",
            "          [1]\n",
            "          [0]\n",
            "    [X22 < 113.800]\n",
            "      [X26 < 0.400]\n",
            "        [0]\n",
            "        [1]\n",
            "      [0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer #using the breast cancer dataset \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import accuracy_score\n",
        "#building a classification tree from scratch and only using numpy, train_test_split and accuracy_score in this practice\n",
        "\n",
        "def entropy(y):\n",
        "    unique, counts = np.unique(y, return_counts=True) #count the unique value occurence in y\n",
        "    p = counts / len(y) #where p is the prob of the unique value in y\n",
        "    return -np.sum(p * np.log2(p)) #the entropy euqation is this \n",
        "\n",
        "def get_gain(X, y, feature_idex, threshold):\n",
        "    left_idexs = X[:, feature_idex] < threshold #if value is smaller than threshold, assign to the left\n",
        "    right_idexs = X[:, feature_idex] >= threshold #otherwise, assign to the right\n",
        "\n",
        "    number_left, number_right = len(y[left_idexs]), len(y[right_idexs]) #check the number of exaples on left and right split \n",
        "    n_total = number_left + number_right\n",
        "\n",
        "    if number_left == 0 or number_right == 0:\n",
        "        return 0 #if either side does not have any example, it is not good for learning, then the info gain is 0\n",
        "\n",
        "    gain = entropy(y) - (number_left/n_total)*entropy(y[left_idexs]) - (number_right/n_total)*entropy(y[right_idexs])\n",
        "    #otherwise, we calculate the info gain by taking the entropy of parent node - weighted entropy left - weighted entropy right\n",
        "\n",
        "    return gain\n",
        "\n",
        "def get_best_feature(X, y, feature_idexs):\n",
        "    best_gain = -1 #initialise best gain\n",
        "    split_idex, split_threshold = None, None\n",
        "\n",
        "    for feature_idex in feature_idexs:\n",
        "        thresholds = np.unique(X[:, feature_idex]) #go through each feature in X\n",
        "        for threshold in thresholds:\n",
        "            gain = get_gain(X, y, feature_idex, threshold) #within each feature, set each unique value to threshold and calculate the info gain\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain \n",
        "                split_idex = feature_idex\n",
        "                split_threshold = threshold\n",
        "\n",
        "    return split_idex, split_threshold #eventually, we will get the best index to split and the best threshold for that feature \n",
        "\n",
        "def build_tree(X, y, depth=0, max_depth=10, min_samples_split=2):\n",
        "    n_samples = X.shape[0] #the number of samples I have in the dataset \n",
        "    n_labels = len(np.unique(y)) # the unique labels in the dataset \n",
        "\n",
        "    #stop conditions\n",
        "    #there are three scenarios: \n",
        "    #when current depth is exceeding max depth (default is 10)\n",
        "    #when all labels in the split is the same (meaning unique value of labe in the node = 1)\n",
        "    #when current split has fewer samples than min_samples_split\n",
        "    if depth >= max_depth or n_labels == 1 or n_samples < min_samples_split:\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        most_common_label = unique[np.argmax(counts)]\n",
        "        return {'leaf_value': most_common_label} #then return a leaf node with a predicted label\n",
        "\n",
        "    # Find the best feature to split on:\n",
        "    feature_idexs = np.random.choice(X.shape[1], int(np.sqrt(X.shape[1])), replace=False) \n",
        "    #size of randomly selected features is the square root of the number of features rounded down to the nearest integer\n",
        "    best_feature, best_threshold = get_best_feature(X, y, feature_idexs)\n",
        "    #use get_best_feature function to get the best feature and the threshold\n",
        "\n",
        "    #split data to left and right based on best feature and threshold\n",
        "    left_idexs = X[:, best_feature] < best_threshold\n",
        "    right_idexs = X[:, best_feature] >= best_threshold\n",
        "\n",
        "    #build tree from the left and right index\n",
        "    left = build_tree(X[left_idexs], y[left_idexs], depth+1, max_depth, min_samples_split)\n",
        "    right = build_tree(X[right_idexs], y[right_idexs], depth+1, max_depth, min_samples_split)\n",
        "\n",
        "    #create a dictionary to represent the current node, that is best feature, best threshold, the left and right sub tree\n",
        "    return {'feature_idx': best_feature, 'threshold': best_threshold,\n",
        "            'left': left, 'right': right}\n",
        "\n",
        "def predict(X, tree):\n",
        "    def traverse_tree(x, node):\n",
        "        if 'leaf_value' in node:\n",
        "            return node['leaf_value'] #if this is a leaf node then tree should return a predicted label \n",
        "\n",
        "        if x[node['feature_idx']] < node['threshold']:\n",
        "            return traverse_tree(x, node['left'])\n",
        "        else:\n",
        "            return traverse_tree(x, node['right'])\n",
        "        #either going to the left or to the right depending on the value and the threshold\n",
        "\n",
        "    return np.array([traverse_tree(x, tree) for x in X])\n",
        "\n",
        "def print_tree(node, depth=0):\n",
        "    indent = '  ' * depth\n",
        "    if 'leaf_value' in node:\n",
        "        print(f\"{indent}[{node['leaf_value']}]\")\n",
        "    else:\n",
        "        print(f\"{indent}[X{node['feature_idx']} < {node['threshold']:.3f}]\")\n",
        "        print_tree(node['left'], depth+1)\n",
        "        print_tree(node['right'], depth+1)\n",
        "\n",
        "#load the breast cancer dataset (breast cancer = bc)\n",
        "X_bc, y_bc = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "#split the dataset into training and testing sets\n",
        "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "\n",
        "#build the decision tree with predefined function build tree \n",
        "tree_bc = build_tree(X_train_bc, y_train_bc, max_depth=5)\n",
        "\n",
        "#make predictions on the testing set\n",
        "y_pred_bc = predict(X_test_bc, tree_bc)\n",
        "\n",
        "#calculate accuracy of the model on the testing set\n",
        "accuracy_bc = accuracy_score(y_test_bc, y_pred_bc)\n",
        "print(f\"Breast cancer dataset accuracy: {accuracy_bc:.3f}\")\n",
        "\n",
        "#print the tree for visualisation \n",
        "print_tree(tree_bc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i4folxZ8_-V"
      },
      "source": [
        "Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRi5Lm7_9CDy",
        "outputId": "eb0ed6e2-d17e-47b3-9c1f-3a5c23542283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error on the testing set: 5978.60270528184\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the mean squared error (MSE) function\n",
        "def mse(y):\n",
        "    return np.mean((y - np.mean(y))**2)\n",
        "\n",
        "# Define a function to split the data into left and right subsets based on a threshold\n",
        "def split(X, y, feature_idx, threshold):\n",
        "    left_indices = np.where(X[:, feature_idx] <= threshold)[0]\n",
        "    right_indices = np.where(X[:, feature_idx] > threshold)[0]\n",
        "    if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "        return None, None, None, None\n",
        "    else:\n",
        "        return X[left_indices], y[left_indices], X[right_indices], y[right_indices]\n",
        "\n",
        "# Define a function to find the best split based on minimizing the MSE\n",
        "def best_split(X, y):\n",
        "    best_feature_idx, best_threshold, best_mse = None, None, np.inf\n",
        "    # Loop through all the features in X\n",
        "    for feature_idx in range(X.shape[1]):\n",
        "        # Find all the unique values of the feature\n",
        "        thresholds = np.unique(X[:, feature_idx])\n",
        "        # Loop through all the unique values of the feature\n",
        "        for threshold in thresholds:\n",
        "            # Split the data into left and right subsets based on the threshold\n",
        "            X_left, y_left, X_right, y_right = split(X, y, feature_idx, threshold)\n",
        "            # Check that the split is valid (i.e. neither the left nor right subset is empty)\n",
        "            if y_left is not None and y_right is not None:\n",
        "                # Calculate the total MSE of the left and right subsets\n",
        "                total_mse = mse(y_left) + mse(y_right)\n",
        "                # Update the best split if the total MSE is lower than the current best MSE\n",
        "                if total_mse < best_mse:\n",
        "                    best_feature_idx, best_threshold, best_mse = feature_idx, threshold, total_mse\n",
        "    return best_feature_idx, best_threshold\n",
        "\n",
        "# Define a function to build the decision tree recursively\n",
        "def build_tree(X, y, depth, max_depth):\n",
        "    # Check if the maximum depth has been reached or if there is no further reduction in MSE\n",
        "    if depth == max_depth or mse(y) == 0:\n",
        "        # Return the mean target value\n",
        "        return np.mean(y)\n",
        "    # Find the best split based on minimizing the MSE\n",
        "    feature_idx, threshold = best_split(X, y)\n",
        "    # Check if there is no valid split\n",
        "    if feature_idx is None:\n",
        "        # Return the mean target value\n",
        "        return np.mean(y)\n",
        "    # Split the data into left and right subsets based on the best split\n",
        "    else:\n",
        "        X_left, y_left, X_right, y_right = split(X, y, feature_idx, threshold)\n",
        "        # Recursively build the left and right subtrees\n",
        "        left_tree = build_tree(X_left, y_left, depth+1, max_depth)\n",
        "        right_tree = build_tree(X_right, y_right, depth+1, max_depth)\n",
        "        # Return the decision node with the best split and the left and right subtrees\n",
        "        return (feature_idx, threshold, left_tree, right_tree)\n",
        "\n",
        "# Define a function to make predictions for a single input using the decision tree\n",
        "\n",
        "\n",
        "def predict_one(x, tree):\n",
        "    # Check if the current node is a leaf node (i.e. a float value)\n",
        "    if isinstance(tree, float):\n",
        "        # Return the mean target value of the leaf node\n",
        "        return tree\n",
        "    \n",
        "    # Check which subtree to go down based on the feature value of the current data point\n",
        "    else:\n",
        "        feature_idx, threshold, left_tree, right_tree = tree\n",
        "        if x[feature_idx] <= threshold:\n",
        "            # Recursively go down the left subtree\n",
        "            return predict_one(x, left_tree)\n",
        "        else:\n",
        "            # Recursively go down the right subtree\n",
        "            return predict_one(x, right_tree)\n",
        "\n",
        "\n",
        "# A function to predict the target values of multiple data points using a decision tree\n",
        "def predict(X, tree):\n",
        "    # Predict the target value of each data point using the predict_one function\n",
        "    return np.array([predict_one(x, tree) for x in X])\n",
        "\n",
        "\n",
        "# A function to build a decision tree for regression\n",
        "def decision_tree_regression(X_train, y_train, max_depth):\n",
        "    # Build the decision tree using the build_tree function\n",
        "    tree = build_tree(X_train, y_train, 0, max_depth)\n",
        "    # Predict the target values of the training set using the predict function\n",
        "    y_pred = predict(X_train, tree)\n",
        "    # Return the decision tree and the predicted target values of the training set\n",
        "    return tree, y_pred\n",
        "\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the decision tree regression model\n",
        "max_depth = 5\n",
        "tree, y_pred_train = decision_tree_regression(X_train, y_train, max_depth)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_test = predict(X_test, tree)\n",
        "\n",
        "# Print the mean squared error on the testing set\n",
        "print(\"Mean squared error on the testing set:\", mse(y_test - y_pred_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
